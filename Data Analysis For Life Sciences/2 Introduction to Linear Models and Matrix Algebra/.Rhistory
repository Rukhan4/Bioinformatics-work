(group.ss = initial.ss - after.group.ss)
group.ms = group.ss / (p - 1)
after.group.ms = after.group.ss / (N - p)
f.value = group.ms / after.group.ms
return(f.value)
})
mean(Fs)
hist(Fs, col="grey", border="white", breaks=50, freq=FALSE)
xs <- seq(from=0,to=6,length=100)
lines(xs, df(xs, df1 = p - 1, df2 = N - p), col="red")
library(UsingR)
data("father.son",package="UsingR")
x <- father.son
avgson <- mean(x$sheight)
avgson
x = father.son$fheight
y = father.son$sheight
mean(y[round(x)==71])
X = matrix(1:1000,100,10)
View(X)
X[25,3]
dim(X)
#Question 2
x = 1:10
Y = cbind(x1=x, x2=2*x, x3=3*x, x4=4*x, x5=5*x)
View(Y)
sum(Y[7,])
dim(Y)
q3 <- matrix(1:60,20,3,byrow=TRUE)
all(q3[,3]%%3==0)
View(q3)
dim(q3)
#Question 5
Z <- seq(10,1,-2)
Z[length(Z)]
#Transpose --------------------------------------------------------------------------
X = matrix(1:15,5,3) #5x3 matrix
X
t(X) #Becomes 3x5 matrix
X #a 5x3 matrix
t(X) #Becomes 3x5 matrix
Z<- matrix(c(1,3,2,1,-2,1,1,1,-1),3,3) #Matrix from book created
#We know answer is (6,2,1) we can try guessing
beta = c(1,2,3)
Z%*%beta #It works!!!
A = matrix(c(1,3,2,1,-2,1,1,1,-1),3,3)
B = matrix(c(6,2,1),3,1)
solve(A)%*%B
g = 9.8
n = 25
h0 = 8848
v0 = 0
time = 10
f = h0+v0*time -0.5*g*time^2
#Question 4
A = matrix(c(2,9,5,2,9,5),2,3)
dim(A)
A[1,2]
#Matrix notation Exercise 6
2.5:6.5
seq(2.5,6.5)
rep(2.5,6.5)
c(2.5,6.5)
#Question 6
q6 = c(seq(1,2),seq(3,4)) #returns a vector
#Question 7
#Det = 0 or
Q7 = matrix(c(2,1,6,3),2,2)
I = matrix(c(1,0,0,1),2,2)
View(I)
solve(Q7,I)
#Question 8
systemeq = matrix(c(1,0,2,1,2,5,1,5,-1),3,3)
View(systemeq)
soln = matrix(c(6,-4,27),3,1)
solve(systemeq)%*%soln
g = 9.8 #gravitational acceleration
n = 25 #observations
tt = seq(0,3.4,len=n) #timeframe
f = 56.67 + 0*tt -0.5*g*tt^2 #56.67 is height of tower of piza
y = f + rnorm(n,sd=1) #Observational error
plot(tt,y,xlab="Time in seconds",ylab = "Distance in Metres")
lines(tt,f,col=2)
#Define function for residual sum of squares
rss = function(Beta0,Beta1,Beta2){
r = y - (Beta0+Beta1*tt+Beta2*tt^2)
sum (r^2)
}
rss
Beta2s = seq(-10,0,len=100)
RSS = sapply(Beta2s,rss,Beta0=55,Beta1=0) #Beta are guesses
plot(Beta2s,RSS,type="l")
tt2 = tt^2
fit = lm(y~tt+tt2)
summary(fit)
#1 create X matrix with covariates as functions
X = cbind(1,tt,tt^2)
#2 Define an arbitrary Beta
Beta = matrix(c(55,0,5),3,1)
#3 do X times Beta using matrix operation to find residuals
r =y - X%*%Beta
#4 get RSS
RSS = t(r)%*%r
RSS
rss(55,0,5)
#using crossprod
RSS2 = crossprod(r)
RSS2
#5 get least squares estimates
Betahat = solve(t(X)%*%X)%*%t(X)%*%y
Betahat
#using crossprod
betahat = solve(crossprod(X))%*%crossprod(X,y)
betahat
#alternative to solve since it is unstable
QR = qr(X)
Q = qr.Q(QR)
R = qr.R(QR)
backsolve(R,crossprod(Q,y))
RNGkind()
#Question 1
g = 9.8 ## meters per second
h0 = 56.67
v0 = 0
n = 25
tt = seq(0,3.4,len=n) ##time in secs, t is a base function
y = 56.67 + v0 *tt  - 0.5* g*tt^2 + rnorm(n,sd=1)
X = cbind(1,tt,tt^2)
A = solve(crossprod(X))%*%t(X)
#rewrite model:
b2 = g/-2
y = h0 + 0*tt + b2*tt^2+rnorm(n,sd=1)
-2*(A%*%y)[3]
set.seed(1)
B = 100000
x = replicate(B,{
b2 = g/-2
y = h0 + 0*tt + b2*tt^2+rnorm(n,sd=1)
g = -2*(A%*%y)[3]
})
sd(x)
betahat = replicate(B,{
y = 56.67  - 0.5*g*tt^2 + rnorm(n,sd=1)
betahats = -2*A%*%y
return(betahats[3])
})
sqrt(mean( (betahat-mean(betahat) )^2))
library(UsingR)
x1 = father.son$fheight
y1 = father.son$sheight
n = length(y)
n = length(y1)
N = 50
set.seed(1)
index = sample(n,N) #Up to 50
sampledat = father.son[index,] #Size 50 cus N is 50
x = sampledat$fheight
y = sampledat$sheight
betahat = lm(y~x)$coef
betahat
#Exercise 1 (sum of squared residuals)
fit = lm(y ~ x)
sumri = sum((y - fit$fitted.values)^2)
#Exercise 2
sigma2 = sumri/(N-2)
X = cbind(rep(1,N),x)
soln = solve(t(X)%*%X)
soln[1,1]
#Exercise 3
diagonal = diag(soln)
ex3 = sigma2*diagonal
sqrt(ex3)
#Question 1
X = matrix(c(1,1,1,1,1,1,0,0,1,1,0,0,0,0,0,0,1,1),nrow=6)
rownames(X) = c("a","a","b","b","c","c")
beta = c(10,3,-3)
fitted = X%*%beta
fitted[3:4,]
fitted[5:6,]
set.seed(1)
N =  50
rang = 10000
index = sample(n,N)
win = replicate(rang,{
sampledat = father.son[index,]
x = sampledat$fheight
y = sampledat$sheight
betahat =  lm(y~x)$coef
})
index = sample(n,N)
#Question 3
library(UsingR)
x = father.son$fheight
y = father.son$sheight
n = length(y)
set.seed(1)
N =  50
rang = 10000
index = sample(n,N)
win = replicate(rang,{
sampledat = father.son[index,]
x = sampledat$fheight
y = sampledat$sheight
betahat =  lm(y~x)$coef
})
sd(win)
betahat = replicate(B,{
index = sample(n,N)
sampledat = father.son[index,]
x = sampledat$fheight
y = sampledat$sheight
lm(y~x)$coef[2]
})
betahat = replicate(rang,{
index = sample(n,N)
sampledat = father.son[index,]
x = sampledat$fheight
y = sampledat$sheight
lm(y~x)$coef[2]
})
sqrt ( mean( (betahat - mean(betahat) )^2 ))
sd(betahat)
#Question 3 HOW TO USE REPLICATE - MONTE CARLO
library(UsingR)
x = father.son$fheight
y = father.son$sheight
n = length(y)
set.seed(1)
N =  50
rang = 10000
index = sample(n,N)
betahat = replicate(rang,{
index = sample(n,N)
sampledat = father.son[index,]
x = sampledat$fheight
y = sampledat$sheight
lm(y~x)$coef[2]
})
sqrt ( mean( (betahat - mean(betahat) )^2 ))
sd(betahat)
#Same-ish answer ^^^^^
-----------------------------------------------------------------------------------------------------------
#Question 4
Covariance = mean((y-mean(y))*(x-mean(X)))
#Same-ish answer ^^^^^
-----------------------------------------------------------------------------------------------------------
#Question 4
Covariance = mean((y-mean(y))*(x-mean(x)))
#Same-ish answer ^^^^^
-----------------------------------------------------------------------------------------------------------
#Question 4
Covariance = mean((y-mean(y))*(x-mean(x)))
#Same-ish answer ^^^^^
-----------------------------------------------------------------------------------------------------------
#Question 4
COVARIANCE = mean((y-mean(y))*(x-mean(x)))
x = sampledat$fheight
y = sampledat$sheight
x = father.son$fheight
y = father.son$sheight
#Same-ish answer ^^^^^
-----------------------------------------------------------------------------------------------------------
#Question 4 #HAVE TO RE-RUN y,x FROM QUESTION 3 AT THE START
COVARIANCE = mean((y-mean(y))*(x-mean(x)))
#Same-ish answer ^^^^^
-----------------------------------------------------------------------------------------------------------
#Question 4 #HAVE TO RE-RUN y,x FROM QUESTION 3 AT THE START
COVARIANCE = mean((x-mean(x))*(y-mean(y)))
#Same-ish answer ^^^^^
-----------------------------------------------------------------------------------------------------------
#Question 4 #HAVE TO RE-RUN y,x FROM QUESTION 3 AT THE START
covariance = mean((Y - mean(Y))*(X-mean(X)))
#Same-ish answer ^^^^^
-----------------------------------------------------------------------------------------------------------
#Question 4 #HAVE TO RE-RUN y,x FROM QUESTION 3 AT THE START
covariance = mean((y - mean(y))*(x-mean(x)))
#Same-ish answer ^^^^^
-----------------------------------------------------------------------------------------------------------
#Question 4 #HAVE TO RE-RUN y,x FROM QUESTION 3 AT THE START
value132 = mean((y - mean(y))*(x-mean(x)))
#Question 4 #HAVE TO RE-RUN y,x FROM QUESTION 3 AT THE START
x = father.son$fheight
y = father.son$sheight
covariance = mean((y - mean(y))*(x-mean(x)))
library(downloader)
url = "https://raw.githubusercontent.com/genomicsclass
/dagdata/master/inst/extdata/femaleMiceWeights.csv"
filename = "femaleMiceWeights.csv"
if (!file.exists(filename)) download (url,filename)
dat = read.csv("femaleMiceWeights.csv")
dat
#Stripchart to visualize weights
stripchart(dat$Bodyweight~dat$Diet,
vertical=TRUE,method="jitter",
main="Bodyweight and Diet")
#Linear model with 1 Variable
levels(dat$Diet)
X = model.matrix(~Diet,data=dat)
X #chooses Diethf because chow is the reference/base level
#Numeric
x = c(1,1,2,2)
f = formula(~x)
model.matrix(f)
class(x)
#Factor
x = factor(c(1,1,2,2))
f =formula(~x)
model.matrix(f)
class(x)
#With 3 levels
x = factor(c(1,1,2,2,3,3))
model.matrix(~x)
#Adding variables with same levels
x = factor(c(1,1,1,1,2,2,2,2))
y = factor(c('a','a','b','b','a','a','b','b'))
model.matrix(~x+y)
#Interaction between variables
model.matrix(~x+y+x:y)
#Effect of base level factor - swaps positions in this case
x = factor(c(1,1,2,2))
model.matrix(~x)
x = relevel(x,"2")
model.matrix(~x)
#Functions / transformations inside a matrix
z = 1:4 #numeric
model.matrix(~z)
model.matrix(~0+z) #removes intercept column
model.matrix(~z + I(z^2))
day = factor(c("A","B","C"))
treated = factor(c(2,2,2))
control = factor(c(2,2,2))
condition = day
model.matrix(~day + condition)
model.matrix(~condition)
model.matrix(~A+B+C+control+treated)
model.matrix(~day)
model.matrix(~day + condition)
dat = read.csv("femaleMiceWeights.csv")
dat
#Stripchart to visualize weights
stripchart(dat$Bodyweight~dat$Diet,
vertical=TRUE,method="jitter",
main="Bodyweight and Diet")
#Linear model with 1 Variable
levels(dat$Diet)
X = model.matrix(~Diet,data=dat)
View(X)
#See Dietchow
dat$Diet = relevel(dat$Diet,ref="hf")
Y = model.matrix(~Diet,data=dat)
Y
X #chooses Diethf because chow is the reference/base level
fit = lm(Bodyweight~Diet,data=dat)
summary(fit) #in my case, summary stands on chow since i releveled dat$Diet
(coefs = coef(fit))
#OR using split method
s = split(dat$Bodyweight, dat$Diet)
mean(s[["chow"]])
mean(s[["hf"]]) - mean(s[["chow"]])
#Results we got from linear model will be equal to results obtained from t-test
summary(fit)$coefficients
s = split(dat$Bodyweight, dat$Diet)
ttest = t.test(s[["chow"]],s[["hf"]],var.equal=TRUE)
ttest
summary(fit)$coefficients[2,3]
ttest$statistic
#Results we got from linear model will be equal to results obtained from t-test
summary(fit)$coefficients
#Good and proper
ttester = t.test(s[["hf"]],s[["chow"]],var.equal=TRUE)
ttester$statistic
(coefs = coef(fit))
#Results we got from linear model will be equal to results obtained from t-test
summary(fit)$coefficients
ttest$statistic
summary(fit)$coefficients[2,3]
ttest
summary(fit)$coefficients[2,3]
ttest$statistic
spider = read.csv("spider_wolff_gorb_2013.csv",skip=1)
spider$log2friction = log2(spider$friction)
boxplot (log2friction~type*leg,data=spider)
fit = lm(friction~type+leg+type:leg,data=spider)
#Question 1
fit2 = lm(log2friction~type*leg,data=spider)
summary(fit2)
#Question 2
anova(fit2)
#Question 3
library(contrast)
L2pullvsL1pull = contrast(fit2,list(leg="L2",type="pull"),list(leg="L1",type="pull"))
L2pullvsL1pull
#Question 4
L2pushvsL1push = contrast(fit2,list(leg="L2",type="push"),list(leg="L1",type="push"))
L2pushvsL1push
View(spider)
#Question 1
species <- factor(c("A","A","B","B"))
condition <- factor(c("control","treated","control","treated"))
p = factor(c(1,2,3,4))
x = model.matrix(~species+condition)
df = data.frame(p,species,condition)
fitter = lm(p~species+condition,data=df)
library(contrast)
contrast(fitter,list(species="B",condition="control"),list(species="A",condition="treated"))$X
#Question 2
spider <- read.csv("spider_wolff_gorb_2013.csv",skip=1)
fit = lm(friction~type+leg,data=spider)
L4vsL2 = contrast(fit,list(leg="L4",type="pull"),list(leg="L2",type="pull"))
L4vsL2
f = matrix(c(1,1,1,1,1,1,0,0,0,1,1,1,0,0,1,0,0,1,1,1,1,0,0,0),6,4)
qr(f)$rank
ncol(f)
#Different hence collinear, expt is designed badly
e = matrix(c(1,1,1,1,0,0,1,1,0,1,0,1,0,0,0,1),4,4)
cat(ncol(e),qr(e)$rank)
sex <- factor(rep(c("female","male"),each=4))
trt <- factor(c("A","A","B","B","C","C","D","D"))
X = model.matrix(~sex+trt)
cat(ncol(X),qr(X)$rank)
#Different hence collinear
Y = 1:8 #Observe some random outcome Y
makeYstar <- function(a,b) Y - X[,2] * a - X[,5] * b
fitTheRest <- function(a,b) {
Ystar <- makeYstar(a,b)
Xrest <- X[,-c(2,5)]
betarest <- solve(t(Xrest) %*% Xrest) %*% t(Xrest) %*% Ystar
residuals <- Ystar - Xrest %*% betarest
sum(residuals^2)
}
fitTheRest(1,2)
betas = expand.grid(-2:8,-2:8)
rss = apply(betas,1,function(x) fitTheRest(x[1],x[2])) #NOTE LOWERCASE x
themin = min(rss)
betas[which(rss==themin),]
#Visualize sums of squared residuals over our grid with imagemat() from rafalib
library(rafalib)
plot(betas[which(rss==themin),])
spider = read.csv("spider_wolff_gorb_2013.csv",skip=1)
fit = lm(friction~type+leg,data=spider)
betahat = coef(fit) #What we want to solve
#matrix work
Y = matrix(spider$friction,ncol=1)
X = model.matrix(~type+leg,data=spider)
#Exercise 1
QR = qr(X)
Q = qr.Q(QR)
Q[1,1]
#Exercise 2
R = qr.R(QR)
R[1,1]
#Exercise 3
Z = crossprod(Q,Y)
Z[1,1]
#Convince myself that QR gives the least squares solution
#R^-1(Q^T Y) compared to betahat
first = diag(R)*Z
betahat = backsolve(R,crossprod(Q,Y))
#Question 1 idk
spider = read.csv("spider_wolff_gorb_2013.csv",skip=1)
x = model.matrix(~type+leg,data=spider)
fit = lm(friction~leg+type,data=spider)
C = matrix(c(0,0,-1,0,1),1,5)
Sigma = sum(fit$residuals^2)/(nrow(x)-ncol(x)*solve(t(x)%*%x))
Sigma[3,5]
#In this last question, we will use Monte Carlo techniques to observe the distribution of the ANOVA's
#"F-value" under the null hypothesis, that there are no differences between groups.
#Suppose we have 4 groups, and 10 samples per group, so 40 samples overall:
N <- 40
p <- 4
group <- factor(rep(1:p,each=N/p))
X <- model.matrix(~ group)
#First generate some random, null data, where the mean is the same for all groups:
Y <- rnorm(N,mean=42,7)
#The base model we wil compare against is simply Y-hat = mean(Y), which we will call mu0,
#and the initial sum of squares is the Y values minus mu0:
mu0 = mean(Y)
initial.ss = sum((Y-mu0)^2)
#We then need to calculate the fitted values for each group, which is simply the mean of each group,
#and the residuals from this model, which we will call after.group.ss for the sum of squares
#after using the group information:
s <- split(Y, group)
after.group.ss <- sum(sapply(s, function(X) sum((X - mean(X))^2)))
#Then the explanatory power of the group variable is the
#initial sum of squares minus the residual sum of squares:
(group.ss <- initial.ss - after.group.ss)
#We calculate the mean of these values, but we divide by terms which remove the number of fitted parameters.
#For the group sum of squares, this is number of parameters used to fit the groups (3, because the intercept is in the initial model).
#For the after group sum of squares, this is the number of samples minus the number of parameters total (So N - 4, including the intercept).
group.ms <- group.ss / (p - 1)
after.group.ms <- after.group.ss / (N - p)
#The F-value is simply the ratio of these mean sum of squares.
f.value <- group.ms / after.group.ms
f.value
#MONTE CARLO SET UP TO GET A MORE ACCURATE f.value
set.seed(1)
Fs = replicate(1000, {
Y = rnorm(N,mean=42,7)
mu0 = mean(Y)
initial.ss = sum((Y - mu0)^2)
s = split(Y, group)
after.group.ss = sum(sapply(s, function(x) sum((x - mean(x))^2)))
(group.ss = initial.ss - after.group.ss)
group.ms = group.ss / (p - 1)
after.group.ms = after.group.ss / (N - p)
f.value = group.ms / after.group.ms
return(f.value)
})
mean(Fs)
hist(Fs, col="grey", border="white", breaks=50, freq=FALSE)
xs <- seq(from=0,to=6,length=100)
lines(xs, df(xs, df1 = p - 1, df2 = N - p), col="red")
